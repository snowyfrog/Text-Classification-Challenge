{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import csv\n",
    "import tensorflow as tf\n",
    "import nltk\n",
    "import gc\n",
    "from gensim.models import Word2Vec\n",
    "from keras.preprocessing import text, sequence\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = pd.read_csv('W.csv')\n",
    "W = W.set_index(W.columns[0])\n",
    "W = np.array(W).astype(np.float32, copy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = pd.read_csv('train_x.csv')\n",
    "val_x = pd.read_csv('val_x.csv')\n",
    "test_x = pd.read_csv('./test_x.csv', sep = ',')\n",
    "ytrain = pd.read_csv('./ytrain.csv', sep = ',')\n",
    "yval = pd.read_csv('./yval.csv', sep = ',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#placeholders and CNN construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_sizes = [1,2,3,4,5]\n",
    "num_filters = 32\n",
    "batch_size = 256\n",
    "#This large batch_size is specially for this case. Usually it is between 64-128.\n",
    "num_filters_total = num_filters * len(filter_sizes)\n",
    "embedding_size = 300\n",
    "sequence_length = 150\n",
    "num_epochs = 3 #Depends on your choice.\n",
    "dropout_keep_prob = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_filters_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_x = tf.placeholder(tf.int32, [None, sequence_length], name = \"input_x\")\n",
    "input_y = tf.placeholder(tf.float32, [None,6], name = \"input_y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_chars = tf.nn.embedding_lookup(W, input_x)\n",
    "embedded_chars_expanded = tf.expand_dims(embedded_chars, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CNN(data):\n",
    "    pooled_outputs = []\n",
    "    \n",
    "    for i, filter_size in enumerate(filter_sizes):\n",
    "        \n",
    "        filter_shape = [filter_size, embedding_size, 1, num_filters]\n",
    "        \n",
    "        w = tf.Variable(tf.truncated_normal(filter_shape,stddev = 0.05), name = \"w\")\n",
    "        b = tf.Variable(tf.truncated_normal([num_filters], stddev = 0.05), name = \"b\")\n",
    "            \n",
    "        conv = tf.nn.conv2d(\n",
    "            data,\n",
    "            w,\n",
    "            strides = [1,1,1,1],\n",
    "            padding = \"VALID\",\n",
    "            name = \"conv\"\n",
    "        )\n",
    "        h = tf.nn.relu(tf.nn.bias_add(conv, b), name = \"relu\")\n",
    "        pooled = tf.nn.max_pool(\n",
    "            h,\n",
    "            ksize = [1, sequence_length - filter_size + 1, 1, 1],\n",
    "            strides = [1,1,1,1],\n",
    "            padding = \"VALID\",\n",
    "            name = \"pool\"\n",
    "        )\n",
    "        \n",
    "        pooled_outputs.append(pooled)\n",
    "    \n",
    "    #return pooled_outputs\n",
    "    h_pool = tf.concat(pooled_outputs, 3)\n",
    "    h_pool_flat = tf.reshape(h_pool, [-1, num_filters_total])\n",
    "    return h_pool_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_pool_flat = CNN(embedded_chars_expanded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_drop = tf.nn.dropout(h_pool_flat, dropout_keep_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In the first dense layer, reduce the node to half.\n",
    "wd1 = tf.Variable(tf.truncated_normal([num_filters_total, int(num_filters_total/2)], stddev=0.05), name = \"wd1\")\n",
    "bd1 = tf.Variable(tf.truncated_normal([int(num_filters_total/2)], stddev = 0.05), name = \"bd1\")\n",
    "layer1 = tf.nn.xw_plus_b(h_drop, wd1, bd1, name = 'layer1') # Do wd1*h_drop + bd1\n",
    "layer1 = tf.nn.relu(layer1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Second dense layer, reduce the outputs to 6.\n",
    "wd2 = tf.Variable(tf.truncated_normal([int(num_filters_total/2),6], stddev = 0.05), name = 'wd2')\n",
    "bd2 = tf.Variable(tf.truncated_normal([6], stddev = 0.05), name = \"bd2\")\n",
    "layer2 = tf.nn.xw_plus_b(layer1, wd2, bd2, name = 'layer2') \n",
    "prediction = tf.nn.sigmoid(layer2)# Make it to be 0-1.\n",
    "#pred_clipped = tf.clip_by_value(prediction, 1e-10, 0.9999999) \n",
    "#For some special loss function clip is necessary. Like log(x)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits = layer2, labels = input_y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = 0.0005).minimize(loss)\n",
    "#Learning rates usually is small for CNN compared with pure neural network. \n",
    "#Need to define a approriate learning rate before you run on the whole dataset.\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.round(prediction), input_y), tf.float32))\n",
    "#correct_prediction = tf.equal(tf.argmax(input_y, 1), tf.argmax(prediction, 1))\n",
    "#accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Blocks and Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define batch generation function.\n",
    "def generate_batch(data, batch_size, num_epochs, shuffle=True):\n",
    "    data = np.array(data)\n",
    "    data_size = len(data)\n",
    "    num_batches_per_epoch = int((len(data)-1)/batch_size) + 1\n",
    "    l = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        l += 1\n",
    "        if shuffle:\n",
    "            shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "            shuffled_data = data[shuffle_indices]\n",
    "        else:\n",
    "            shuffled_data = data\n",
    "        for batch_num in range(num_batches_per_epoch):\n",
    "            start_index = batch_num * batch_size\n",
    "            end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "            yield shuffled_data[start_index:end_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For Test data. Can use generate_batch function.\n",
    "def blocks(data, block_size):\n",
    "    data = np.array(data)\n",
    "    data_size = len(data)\n",
    "    nums = int((data_size-1)/block_size) + 1\n",
    "    for block_num in range(nums):\n",
    "        if block_num == 0:\n",
    "            print(\"prediction start!\")\n",
    "        start_index = block_num * block_size\n",
    "        end_index = min((block_num + 1) * block_size, data_size)\n",
    "        print(end_index)\n",
    "        yield data[start_index:end_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training and evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ready to predict test data.\n",
    "x_train = pd.DataFrame(train_x).append(pd.DataFrame(val_x))\n",
    "y_train = pd.DataFrame(ytrain).append(pd.DataFrame(yval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The reason to create 7 different batches here is because \n",
    "#I want to make the data totally shuffled to reduce the risk that one batch have all 0.\n",
    "\n",
    "batch1 = generate_batch(list(zip(np.array(x_train), y_train['toxic'], y_train['severe_toxic'], y_train['obscene'], y_train['threat'], y_train['insult'], y_train['identity_hate'])), 1000, 1)\n",
    "batch2 = generate_batch(list(zip(np.array(x_train), y_train['toxic'], y_train['severe_toxic'], y_train['obscene'], y_train['threat'], y_train['insult'], y_train['identity_hate'])), batch_size, 1)\n",
    "batch3 = generate_batch(list(zip(np.array(x_train), y_train['toxic'], y_train['severe_toxic'], y_train['obscene'], y_train['threat'], y_train['insult'], y_train['identity_hate'])), batch_size, 1)\n",
    "batch4 = generate_batch(list(zip(np.array(x_train), y_train['toxic'], y_train['severe_toxic'], y_train['obscene'], y_train['threat'], y_train['insult'], y_train['identity_hate'])), batch_size, 1)\n",
    "batch5 = generate_batch(list(zip(np.array(x_train), y_train['toxic'], y_train['severe_toxic'], y_train['obscene'], y_train['threat'], y_train['insult'], y_train['identity_hate'])), batch_size, 1)\n",
    "batch6 = generate_batch(list(zip(np.array(x_train), y_train['toxic'], y_train['severe_toxic'], y_train['obscene'], y_train['threat'], y_train['insult'], y_train['identity_hate'])), batch_size, 1)\n",
    "batch7 = generate_batch(list(zip(np.array(x_train), y_train['toxic'], y_train['severe_toxic'], y_train['obscene'], y_train['threat'], y_train['insult'], y_train['identity_hate'])), batch_size, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_blocks = blocks(list(np.array(test_x)), 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_bag = [batch1,batch2,batch3]#,batch4,batch5,batch6]#,batch7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_op = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    sess.run(init_op)\n",
    "    i = 0\n",
    "    for batches in batch_bag:\n",
    "        i += 1\n",
    "        print('Epoch: ' + str(i) + ' start!')\n",
    "        avg_acc = 0\n",
    "        avg_loss = 0\n",
    "        for batch in batches:\n",
    "            batch = pd.DataFrame(batch, columns = ['a','b','c','d','e','g','f'])\n",
    "            x_batch = pd.DataFrame(list(batch['a']))\n",
    "            y_batch = batch.loc[:, batch.columns != 'a']\n",
    "            _,c, acc = sess.run([optimizer, loss, accuracy],feed_dict = {input_x: x_batch, input_y: y_batch})\n",
    "            avg_loss += c\n",
    "            avg_acc += acc\n",
    "            #print(str(c) + ' and ' + str(acc))\n",
    "            #print('pred_train')\n",
    "            #print(prediction.eval({input_x: x_batch, input_y: y_batch}))\n",
    "        avg_loss = avg_loss/624\n",
    "        avg_acc = avg_acc/624\n",
    "        print('Epoch:' + str(i) + ' loss is ' + str(avg_loss) + ', accuracy is ' + str(avg_acc))\n",
    "        #print('Evaluation Accuracy: ')\n",
    "        #print(accuracy.eval({input_x: val_x, input_y: yval}))\n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "    for block in test_blocks:\n",
    "        block = pd.DataFrame(block)\n",
    "        pred = sess.run(prediction, feed_dict = {input_x: block})\n",
    "        df = df.append(pd.DataFrame(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]] = np.array(df)\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.round().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.loc[:,sub.columns != 'id'].round().mean()\n",
    "#Results from keras epoch = 3. Accuracy =  98.20%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import csv\n",
    "import tensorflow as tf\n",
    "import nltk\n",
    "import gc\n",
    "from gensim.models import Word2Vec\n",
    "from keras.preprocessing import text, sequence\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PREPROCESSING\n",
    "#This dataset is from Kaggle Competition, Toxic Comment Classification Challenge, \n",
    "#that train dataset contains 159571 rows and 8 columns, which are id, comment_text, \n",
    "#toxic, sever_toxic, obscene, threat, insult and identity_hate.\n",
    "#The test dataset has over 150000 records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('train.csv') \n",
    "df_test = pd.read_csv('test.csv')\n",
    "train_input = df_train['comment_text']\n",
    "test_input = df_test['comment_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to read the FastText Pre-trained Word Embedding in to a dictionary.\n",
    "def get_coefs(word, *arr): \n",
    "    return word, np.asarray(arr, dtype='float32')\n",
    "\n",
    "embeddings_index = dict(get_coefs(*o.rstrip().rsplit(' ')) for o in open('./wiki.en.vec'))\n",
    "del embeddings_index['2519370'] \n",
    "# The first row of the file is useless, so delete it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(embeddings_index) \n",
    "#FastText Word Embedding file contains 2500000 words including punctuations.\n",
    "#It doesn't contains 0-9 and words like I'm, can't and etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 100000\n",
    "maxlen = 150 \n",
    "#Set the max length of each comment. If it is longer than 150 then cut if off,\n",
    "#if it is shorter than 150 then pad it up to 150.\n",
    "#This max length can be choosen in different ways. \n",
    "#Here it is a number that near 80 percentile of all comment length in training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data cleaning function\n",
    "def clean(string):\n",
    "    string = re.sub(r'\\n', ' ', string)\n",
    "    string = re.sub(r'\\t', ' ', string)\n",
    "    string = re.sub(\"[^A-Za-z\\(\\)\\,\\.\\?\\'\\!]\", \" \", string)\n",
    "    string = re.sub(\"\\'m\", ' am ', string)\n",
    "    string = re.sub(\"\\'s\", ' is ', string)\n",
    "    string = re.sub(\"can\\'t\", 'cannot ', string)\n",
    "    string = re.sub(\"n\\'t\", ' not ', string)\n",
    "    string = re.sub(\"\\'ve\", ' have ', string)\n",
    "    string = re.sub(\"\\'re\", ' are ', string)\n",
    "    string = re.sub(\"\\'d\", \" would \", string)\n",
    "    string = re.sub(\"\\'ll\", \" will \", string)\n",
    "    string = re.sub(\"\\,\", \" , \", string)\n",
    "    string = re.sub(\"\\'\", \" ' \", string)\n",
    "    string = re.sub(\"\\.\", \" . \", string)\n",
    "    string = re.sub(\"\\!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" ( \", string)\n",
    "    string = re.sub(r\"\\)\", \" ) \", string)\n",
    "    string = re.sub(r\"\\?\", \" ? \", string)\n",
    "    string = re.sub(r'\\s{2,}', ' ', string.lower())\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train_input.apply(clean)\n",
    "y_train = df_train[['toxic','severe_toxic',\"obscene\", \"threat\", \"insult\", \"identity_hate\"]]\n",
    "x_test = test_input.apply(clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#After data clean there might be some record have nothing in comment_text, fill with a word.\n",
    "x_train = x_train.fillna('fillna')\n",
    "x_test = x_test.fillna('fillna')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the dictionary whose keys contains all words in train dataset that also shown \n",
    "#in FastText word embeddings.\n",
    "lst = []\n",
    "for line in x_train:\n",
    "    lst += line.split()\n",
    "    \n",
    "count = Counter(lst)\n",
    "for k in list(count.keys()):\n",
    "    if k not in embeddings_index:\n",
    "        del count[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = dict(sorted(count.items(), key=lambda x: -x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = {k:v for (k,v) in count.items() if v >= 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = dict(zip(list(count.keys()),range(1,79101 + 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = {}\n",
    "for key in count:\n",
    "    embedding_matrix[key] = embeddings_index[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create teh word embedding matrix where the first element is all zeros which is for word\n",
    "#that is not shown and the padding elements.\n",
    "W = np.zeros((1,300))\n",
    "W = np.append(W, np.array(list(embedding_matrix.values())),axis=0)\n",
    "W = W.astype(np.float32, copy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Same Step for text dataset.\n",
    "lst = []\n",
    "for line in x_test:\n",
    "    lst += line.split()\n",
    "    \n",
    "count_test = Counter(lst)\n",
    "for k in list(count_test.keys()):\n",
    "    if k not in embedding_matrix:\n",
    "        del count_test[k]\n",
    "    else:\n",
    "        count_test[k] = count[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Release memory.\n",
    "del lst\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make the train dataset to be a sequence of ids of words.\n",
    "for i in range(len(x_train)):\n",
    "    temp = x_train[i].split()\n",
    "    for word in temp[:]:\n",
    "        if word not in count:\n",
    "            temp.remove(word)\n",
    "    for j in range(len(temp)):\n",
    "        temp[j] = count[temp[j]]\n",
    "    x_train[i] = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create evaluation dataset.\n",
    "Xtrain, Xval, ytrain, yval = train_test_split(x_train, y_train, train_size=0.80, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pad sequence to 150 length.\n",
    "train_x = sequence.pad_sequences(list(Xtrain), maxlen = maxlen)\n",
    "val_x = sequence.pad_sequences(list(Xval), maxlen = maxlen)\n",
    "test_x = sequence.pad_sequences(list(x_test), maxlen = maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save for easy loading.\n",
    "pd.DataFrame(W).to_csv('./W.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del embeddings_index\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save file.\n",
    "pd.DataFrame(train_x).to_csv('./train_x.csv', sep = ',', index = False)\n",
    "pd.DataFrame(val_x).to_csv('./val_x.csv', sep = ',', index = False)\n",
    "pd.DataFrame(test_x).to_csv('./test_x.csv', sep = ',', index = False)\n",
    "pd.DataFrame(ytrain).to_csv('./ytrain.csv', sep = ',', index = False)\n",
    "pd.DataFrame(yval).to_csv('./yval.csv', sep = ',', index = False)\n",
    "pd.DataFrame(x_train).to_csv('./x_train.csv', sep = ',', index = False)\n",
    "pd.DataFrame(y_train).to_csv('./y_train.csv', sep = ',', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Finance)",
   "language": "python",
   "name": "finance"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
